---
layout: post
title: "Learning to play Can't Stop with a Deep Q Network"
author: "Yu Jia Cheong"
categories: personal
tags: [reinforcement learning, games]
image: dice.jpg
youtubeId: bvNVi8pXZKQ
---

What with the COVID19 related confinement, I have been playing a lot of board games on [board game arena](https://boardgamearena.com/) with friends, colleagues, you name it. When you first create an account, the site takes you through the tutorial of a simple probabilistic risk vs gain evaluation game called [Can't Stop](https://en.wikipedia.org/wiki/Can't_Stop_(board_game)). Having a little more time on my hands during weekends than usual, I decided to train a reinforcement learning agent to learn to play this game, for which the full code can be found [here](https://github.com/yujia21/board_games_rl).

# Can't Stop!
The easiest way to understand the mechanics of the game if you don't already know it is probably to watch this [tutorial](https://www.youtube.com/watch?v=VUGvOQatVDc). Briefly though, the set up is as follows. This 2 to 4-player game comprises of 11 columns running from 2 to 12, and the goal is to be the first to claim three columns, which is done by moving your counter from the bottom to the top of the column. Some columns are easier than others to complete.

![Game board of Can't Stop (Source: wiki)](/assets/img/board_games/CantStop.png)

At each turn, the players roll 4 dice and choose columns corresponding to the result when they separate the dice into two pairs and sum each pair. After choosing his columns, the player then decides whether to end his turn or continue rolling. The catch is that in any turn, players can move up a maximum of three different columns, and loses all advances made should he perform a roll where he is unable to advance on any of the three columns in that turn. Players therefore aim to stop and lock in their positions on these columns at the optimal moment. For every subsequent turn, previous locked in advances are kept, and players can choose new columns to move up.

The game therefore involves balancing the risk and reward of continuing to roll, with some subtleties in the multi-player dynamic that involve whether columns where other players have already established a headstart are worth pursuing.

# Reinforcement Learning
With the [stunning victory of AlphaGo over Lee Sedol](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol) in the game of Go, long thought to be a benchmark in the development of artificial intelligence, it seems as though everyone in the machine learning community was talking about reinforcement learning for a while, especially with respect to games with well defined rules. As reinforcement learning has already been explained much more capably elsewhere, and I will give only a brief summary here.

The idea behind this branch of machine learning is actually simple. Just as a child learns to avoid hot objects after experiencing the pain of burning, or to reach for sweets because he remembers the pleasure from eating it, the idea is to train an algorithm with punishments and rewards so it knows how to react to an environment optimally. Some terms generally used in setting up a problem to solve in reinforcement learning:

The **agent** is the algorithm we train, who will take **actions** in an **environment**, whose **state** will change as a result of the action. In classical reinforcement learning, the agent then receive the following information about this change: and **observation** and a **reward**. The agent then takes the next action, and the cycle repeats. Over time, as the agent collects a history of the observations and rewards resulting from its different actions, it should learn to perform the correct actions upon receiving an observation of the state.

Notice that in general, the agent does not need to know anything about the game he is playing beforehand. In the example of Go, the algorithm would probably play completely randomly at the start, then learn over time which strategies and moves would lead to gaining points during the game, as well as a final victory at the end of the game. The beauty of the reinforcement learning is that the agent learns to associate the rewards not only with the last action taken, but also with the whole history of actions and observations that lead to this point. This means that the agent can learn to associate long term rewards with actions that might be potentially bring less reward at the moment (as an analogy, we can choose to suffer the brief pain of exercise and not indulge in the pleasures of junk food, in exchange for good health in the long term.)

## Deep Q Network
The DQN algorithm is used where action choices are discrete, and basically works as follows: the model comprises of a neural network that outputs a "score" for each of the possible actions given an observation. The agent then chooses the action to take based on these scores - when in production, it should always choose the action with the best score, if still in training, it might at times choose an action with smaller scores in case it might lead to future actions with higher scores. Different ways of choosing an action can be decided based on how we want to prioritize **exploration vs exploitation** - whether we want to exploit the actions we already know are good, or to explore more in case there are better options.

(The name deep Q network comes from the deep neural network involved, and the reward function Q it tries to approximate, that takes into account future possible evolutions of the actions and not merely the current resulting reward.)

# Implementation
## Agent
I decided to use the [Keras RL](https://github.com/keras-rl/keras-rl/tree/master/rl) implementation of a DQN agent to learn to play the game. As Keras RL doesn't support multi-player environments out of the box, I adapted the Interleaved Agent as developed by [velochy](https://github.com/velochy/rl-bargaining/blob/master/interleaved.py), which allow the different agents to learn to play together, and to be decoupled from each other at the end.

As this is a multi-phase game on top of being a multi-player game, I chose to use the same agent to perform both the "stop or continue" and "choose which dice pair" phases to simplify training. This means that not only does the agent have to learn how to complete the game without knowing the rules of the game, it also has to learn how to respond correctly to the phase - the agent cannot choose to "stop or continue" when it is in the "choose which dice pair" phase and vice versa, otherwise he receives a penalty and the game would end. In addition, unlike human beings who are able to infer patterns from a wider understanding of the world, the agent has no visual or pattern relating clues - it does not know it is playing a board game with logical rules, cannot observe the design of the board and summise that choosing a pair of dice that sum to 7 corresponds to moving up the column 7, therefore this pattern should hold true for other sums and columns.

Practically the agent will choose an action corresponding to the numbers 0 to 7, with 0 and 1 representing stopping and continuing, and 2-7 representing the 6 possible choices of die sums. If the player chooses a die sum and the other die sum in the pair is still feasible (such as at the start of the turn when the player has yet to advance on any column), both die sums will be played (that is, choosing 2 or 3 would be equivalent when both die sums represented by 2 and 3 are feasible).

Alternatively, I don't think that it would be too difficult to train two separate agents instead as these choices are relatively independent, with one agent evaluating the risk of stopping and continuing, and the other agent choosing the optimal columns to continue.

## Environment
The environment I implemented uses Open AI's [Gym](https://github.com/openai/gym) base environment class for compatibility with Keras RL's training structure. The environment basically codes all of the rules of the games into how it changes the state of the board after a player chooses an action, and how it generates dice rolls. The observations that the environment returns at each turn to the agent comprises of the following:
- the phase of the game (choose to stop or continue, choose dice pair)
- the positions of the player on each column of the board
- the positions of the other 3 players on the columns of the board. If less than 4 players are playing, the remaining empty slots are simply always 0. This allows for the same agent trained in a 4 player set up to play in games with 2 or 3 player as well.
- the three columns on which the player is advancing in this turn

In theory, the only reward of this game comes at the end when the game is won. However, as I was also teaching my agents to choose the right actions corresponding to the right phases, the penalties associated with choosing the wrong actions for a given phase means that the agent would quickly learn actions that result in zero penalties. I would think it possible to find the parameters allowing one to train agents on a "pure" version where the only reward is in winning three columns, but trying this out showed that practically speaking, the agents took too long to learn to move up the columns. I added in some partial rewards for completing a single column, and observed that agents then stopped immediately after performing one roll and thus advanced up the columns slowly, although a second roll is always non risky! In ordr to incentivize learning to advance multiple steps in a single turn, more partial rewards were given for when a player advances more in a single turn.

# Results
The results of a quick round of training over 100,000 steps already proved promising; the agents are able to complete the game and have learned to continue rolling the die in most risk-free situations, although at times they still seem to stop too early. The following video shows four copies of the same agent with the same neural network weights competing with each other.

{% include youtubePlayer.html id=page.youtubeId %}

I'm not sure how much my agents have actually learned - whether they're always making the best choices (for example, whether a winning move will always be chosen). Doing this analysis would probably result in more complex implementations of Keras Callbacks over the testing environment, but for now I'm content to know that my agent has learned to play the game enough to complete it reasonably quickly. Perhaps on a next weekend when I have some more time on my hand!

Tweaking the rewards structure and learning parameters of the agents could also yield better results. It could be interesting to think of a way for the agents to share knowledge with each other. In the current implementation, each agent is blind to the rolls and actions taken by other agents when it is not their turn, only observing the resulting board changes at the start of their turn. This is clearly different from human players being able to observe the strategies of the friends they are playing with (some people who prefer to go for the rare but easy to complete columns, 2 and 12) and adapt their own strategy accordingly.

Of course, since Can't Stop is pretty straightforward probabilistically speaking, there's no actual need to use reinforcement learning to create an AI to play the game, and a calculation of the expected gains conditioned on the board situation at each turn should yield a bot with decent performance.
